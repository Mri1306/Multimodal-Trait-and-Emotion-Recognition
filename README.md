# Multimodal-Trait-and-Emotion-Recognition
ğŸ“Œ Overview

This repository implements a modular, agentic AI pipeline for inferring Big Five personality traits and emotional states from multimodal interview-style data enriched with behavioral metadata (response time, body language, speech features).
The system integrates three coordinated agents:

Perception Agent â€“ Emotion classification

Inference Agent â€“ Big Five trait estimation

Dialogue Agent â€“ Personality- and emotion-aware response generation

A retrieval-augmented memory module connects the agents, enabling context continuity and adaptive, psychologically informed dialogue.

âœ¨ Features

Multimodal input enrichment (text + behavioral metadata)

Agentic workflow loop: Observe â†’ Reflect â†’ Act â†’ Self-Audit

Dual LLM backbone support: LLaMA 3.2 1B and Falcon-RW-1B

Big Five personality trait estimation (OCEAN model)

Emotionally adaptive dialogue generation

Retrieval-Augmented Generation (RAG) for contextual grounding

Benchmark evaluation with latency, diversity, and empathy metrics


ğŸ— System Architecture
User Input + Metadata
       â”‚
       â–¼
 Perception Agent  â†’  Emotion Classification
       â”‚
       â–¼
 Inference Agent   â†’  Big Five Trait Scoring
       â”‚
       â–¼
 Retrieval Memory  â†’  Context from past interactions
       â”‚
       â–¼
 Dialogue Agent    â†’  Empathic, trait-aware response
