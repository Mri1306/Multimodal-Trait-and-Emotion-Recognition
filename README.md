# Multimodal-Trait-and-Emotion-Recognition
ðŸ“Œ Overview

This repository implements a modular, agentic AI pipeline for inferring Big Five personality traits and emotional states from multimodal interview-style data enriched with behavioral metadata (response time, body language, speech features).
The system integrates three coordinated agents:

Perception Agent â€“ Emotion classification

Inference Agent â€“ Big Five trait estimation

Dialogue Agent â€“ Personality- and emotion-aware response generation

A retrieval-augmented memory module connects the agents, enabling context continuity and adaptive, psychologically informed dialogue.

âœ¨ Features

Multimodal input enrichment (text + behavioral metadata)

Agentic workflow loop: Observe â†’ Reflect â†’ Act â†’ Self-Audit

Dual LLM backbone support: LLaMA 3.2 1B and Falcon-RW-1B

Big Five personality trait estimation (OCEAN model)

Emotionally adaptive dialogue generation

Retrieval-Augmented Generation (RAG) for contextual grounding

Benchmark evaluation with latency, diversity, and empathy metrics


ðŸ— System Architecture

User Input + Metadata -----> Perception Agent  â†’  Emotion Classification -----> Inference Agent   â†’  Big Five Trait Scoring -----> Retrieval Memory  â†’  Context from past interactions -----> Dialogue Agent    â†’  Empathic, trait-aware response
